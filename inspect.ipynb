{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'dev', 'test'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/iemocap/data_iemocap.pkl', 'rb') as f:\n",
    "    x = pkl.load(f)\n",
    "    \n",
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/iemocap_gc/data_iemocap_gc.pkl\")\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 108/108 [00:00<00:00, 281182.39it/s]\n",
      "dev: 100%|██████████| 12/12 [00:00<00:00, 53830.64it/s]\n",
      "test: 100%|██████████| 31/31 [00:00<00:00, 60335.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# from utils import set_seed\n",
    "\n",
    "def load_iemocap():\n",
    "    path = \"data/iemocap_roberta/iemocap_roberta.pkl\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        unsplit = pickle.load(f)\n",
    "    \n",
    "    speaker_to_idx = {\"M\": 0, \"F\": 1}\n",
    "\n",
    "    data = {\n",
    "        \"train\": [], \"dev\": [], \"test\": [],\n",
    "    }\n",
    "    trainVid = list(unsplit[\"trainVid\"])\n",
    "    random.shuffle(trainVid)\n",
    "    testVid = list(unsplit[\"testVid\"])\n",
    "\n",
    "    dev_size = int(len(trainVid) * 0.1)\n",
    "    \n",
    "    spliter = {\n",
    "        \"train\": trainVid[dev_size:],\n",
    "        \"dev\": trainVid[:dev_size],\n",
    "        \"test\": testVid\n",
    "    }\n",
    "\n",
    "    for split in data:\n",
    "        for uid in tqdm(spliter[split], desc=split):\n",
    "            data[split].append(\n",
    "                {\n",
    "                    \"uid\" : uid,\n",
    "                    \"speakers\" : [speaker_to_idx[speaker] for speaker in unsplit[\"speaker\"][uid]],\n",
    "                    \"labels\" : unsplit[\"label\"][uid],\n",
    "                    \"text\": unsplit[\"text\"][uid],\n",
    "                    \"audio\": unsplit[\"audio\"][uid],\n",
    "                    \"visual\": unsplit[\"visual\"][uid],\n",
    "                    \"sentence\" : unsplit[\"sentence\"][uid],\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    return data\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.batch_size = args.batch_size\n",
    "        self.num_batches = math.ceil(len(data)/ self.batch_size)\n",
    "        self.dataset = args.dataset\n",
    "        self.embedding_dim = args.embedding_dim[self.dataset]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch = self.raw_batch(index)\n",
    "        return self.padding(batch)\n",
    "\n",
    "    def raw_batch(self, index):\n",
    "        assert index < self.num_batches, \"batch_idx %d > %d\" % (index, self.num_batches)\n",
    "        batch = self.data[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        return batch\n",
    "\n",
    "    def padding(self, samples):\n",
    "        batch_size = len(samples)\n",
    "        text_len_tensor = torch.tensor([len(s[\"text\"]) for s in samples]).long()\n",
    "        mx = torch.max(text_len_tensor).item()\n",
    "        \n",
    "        audio_tensor = torch.zeros((batch_size, mx, self.embedding_dim['a']))\n",
    "        text_tensor = torch.zeros((batch_size, mx, self.embedding_dim['t']))\n",
    "        visual_tensor = torch.zeros((batch_size, mx, self.embedding_dim['v']))\n",
    "\n",
    "        speaker_tensor = torch.zeros((batch_size, mx)).long()\n",
    "\n",
    "        labels = []\n",
    "        utterances = []\n",
    "        for i, s in enumerate(samples):\n",
    "            cur_len = len(s[\"text\"])\n",
    "            utterances.append(s[\"sentence\"])\n",
    "\n",
    "            tmp_t = []\n",
    "            tmp_a = []\n",
    "            tmp_v = []\n",
    "            for t, a, v in zip(s[\"text\"], s[\"audio\"], s[\"visual\"]):\n",
    "                tmp_t.append(torch.tensor(t))\n",
    "                tmp_a.append(torch.tensor(a))\n",
    "                tmp_v.append(torch.tensor(v))\n",
    "                \n",
    "            tmp_a = torch.stack(tmp_a)\n",
    "            tmp_t = torch.stack(tmp_t)\n",
    "            tmp_v = torch.stack(tmp_v)\n",
    "\n",
    "            text_tensor[i, :cur_len, :] = tmp_t\n",
    "            audio_tensor[i, :cur_len, :] = tmp_a\n",
    "            visual_tensor[i, :cur_len, :] = tmp_v\n",
    "            \n",
    "            speaker_tensor[i, :cur_len] = torch.tensor(s[\"speakers\"])\n",
    "\n",
    "            labels.extend(s[\"labels\"])\n",
    "\n",
    "        label_tensor = torch.tensor(labels).long()\n",
    "        \n",
    "\n",
    "        data = {\n",
    "            \"length\": text_len_tensor,\n",
    "            \"tensor\": {\n",
    "                \"t\": text_tensor,\n",
    "                \"a\": audio_tensor,\n",
    "                \"v\": visual_tensor,\n",
    "            },\n",
    "            \"speaker_tensor\": speaker_tensor,\n",
    "            \"label_tensor\": label_tensor,\n",
    "            \"utterance_texts\": utterances,\n",
    "        }\n",
    "\n",
    "        return data\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "\n",
    "\n",
    "data = load_iemocap()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'dev', 'test'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to data/iemocap_roberta/data_iemocap_roberta.pkl\n"
     ]
    }
   ],
   "source": [
    "output_path = \"data/iemocap_roberta/data_iemocap_roberta.pkl\"\n",
    "\n",
    "# Dump the 'data' dictionary into a pickle file\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
